{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Байесовская классификация спам/не спам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn.feature_extraction.text as tx, sklearn.naive_bayes as nb, sklearn.linear_model as lm, sklearn.ensemble as en, sklearn.metrics as me, sklearn.model_selection as ms\n",
    "from warnings import filterwarnings as fw\n",
    "fw('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Узнаем вариант"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('А')%3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прочитаем корпус текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>605</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: enron methanol ; meter # : 988291\\r\\n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2349</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: hpl nom for january 9 , 2001\\r\\n( see...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3624</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: neon retreat\\r\\nho ho ho , we ' re ar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4685</td>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: photoshop , windows , office . cheap ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2030</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: re : indian springs\\r\\nthis deal is t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5166</th>\n",
       "      <td>1518</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: put the 10 on the ft\\r\\nthe transport...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5167</th>\n",
       "      <td>404</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: 3 / 4 / 2000 and following noms\\r\\nhp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5168</th>\n",
       "      <td>2933</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: calpine daily gas nomination\\r\\n&gt;\\r\\n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5169</th>\n",
       "      <td>1409</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: industrial worksheets for august 2000...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5170</th>\n",
       "      <td>4807</td>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: important online banking alert\\r\\ndea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5171 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0 label                                               text  \\\n",
       "0            605   ham  Subject: enron methanol ; meter # : 988291\\r\\n...   \n",
       "1           2349   ham  Subject: hpl nom for january 9 , 2001\\r\\n( see...   \n",
       "2           3624   ham  Subject: neon retreat\\r\\nho ho ho , we ' re ar...   \n",
       "3           4685  spam  Subject: photoshop , windows , office . cheap ...   \n",
       "4           2030   ham  Subject: re : indian springs\\r\\nthis deal is t...   \n",
       "...          ...   ...                                                ...   \n",
       "5166        1518   ham  Subject: put the 10 on the ft\\r\\nthe transport...   \n",
       "5167         404   ham  Subject: 3 / 4 / 2000 and following noms\\r\\nhp...   \n",
       "5168        2933   ham  Subject: calpine daily gas nomination\\r\\n>\\r\\n...   \n",
       "5169        1409   ham  Subject: industrial worksheets for august 2000...   \n",
       "5170        4807  spam  Subject: important online banking alert\\r\\ndea...   \n",
       "\n",
       "      label_num  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             1  \n",
       "4             0  \n",
       "...         ...  \n",
       "5166          0  \n",
       "5167          0  \n",
       "5168          0  \n",
       "5169          0  \n",
       "5170          1  \n",
       "\n",
       "[5171 rows x 4 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pd.read_csv('Email spam 2 .csv')\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проанализируем количество слов по классам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего строк: 5171 \n",
      "Из них 1499 spam и 3672 ham \n",
      "Всего слов:  1083244 \n",
      "из них 320863 spam и 762381 ham\n"
     ]
    }
   ],
   "source": [
    "cnt, cntS, cntH = 0, 0, 0\n",
    "for i in corpus['text']:\n",
    "    cnt += len(i.split(' '))\n",
    "for i in corpus[corpus['label']=='spam']['text']:\n",
    "    cntS += len(i.split(' '))\n",
    "for i in corpus[corpus['label']=='ham']['text']:\n",
    "    cntH += len(i.split(' '))\n",
    "print('Всего строк:', corpus.shape[0], '\\nИз них', \n",
    "    corpus['label'].value_counts()['spam'], 'spam и',corpus['label'].value_counts()['ham'], 'ham', \n",
    "    '\\nВсего слов: ', cnt, '\\nиз них', cntS, 'spam и', cntH, 'ham')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучающая выборка: (4136,) \n",
      "Тестовая выборка (1035,)\n"
     ]
    }
   ],
   "source": [
    "xtrain, xtest, ytrain, ytest = ms.train_test_split(corpus['text'],corpus['label'], test_size=0.2)\n",
    "print('Обучающая выборка:', xtrain.shape,'\\nТестовая выборка', xtest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- -- \n",
    "Используем все слова с учетом регистра:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего 50448 токенов\n"
     ]
    }
   ],
   "source": [
    "cv = tx.CountVectorizer(lowercase=False)\n",
    "cv.fit(corpus['text'])\n",
    "print('Всего', len(cv.vocabulary_.keys()), 'токенов')\n",
    "\n",
    "tr_xtrain = cv.transform(xtrain)\n",
    "tr_xtest = cv.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Логистическая регрессия: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.98472   0.97123   0.97793       730\n",
      "        spam    0.93333   0.96393   0.94839       305\n",
      "\n",
      "    accuracy                        0.96908      1035\n",
      "   macro avg    0.95903   0.96758   0.96316      1035\n",
      "weighted avg    0.96958   0.96908   0.96922      1035\n",
      " Случайный лес: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.97934   0.97397   0.97665       730\n",
      "        spam    0.93851   0.95082   0.94463       305\n",
      "\n",
      "    accuracy                        0.96715      1035\n",
      "   macro avg    0.95893   0.96240   0.96064      1035\n",
      "weighted avg    0.96731   0.96715   0.96721      1035\n",
      " Наивный Байес: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.99164   0.97534   0.98343       730\n",
      "        spam    0.94322   0.98033   0.96141       305\n",
      "\n",
      "    accuracy                        0.97681      1035\n",
      "   macro avg    0.96743   0.97784   0.97242      1035\n",
      "weighted avg    0.97737   0.97681   0.97694      1035\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr =  lm.LogisticRegression()\n",
    "rfc = en.RandomForestClassifier()\n",
    "cnb = nb.ComplementNB()\n",
    "\n",
    "lr.fit(tr_xtrain,ytrain)\n",
    "rfc.fit(tr_xtrain,ytrain)\n",
    "cnb.fit(tr_xtrain,ytrain)\n",
    "\n",
    "pred_lr = lr.predict(tr_xtest)\n",
    "pred_rfc = rfc.predict(tr_xtest)\n",
    "pred_cnb = cnb.predict(tr_xtest)\n",
    "\n",
    "print('Логистическая регрессия: \\n', me.classification_report(ytest, pred_lr, digits=5),\n",
    "    'Случайный лес: \\n', me.classification_report(ytest, pred_rfc, digits=5),\n",
    "    'Наивный Байес: \\n', me.classification_report(ytest, pred_cnb, digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- -- \n",
    "Используем все слова, но в одном регистре:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего 50447 токенов\n"
     ]
    }
   ],
   "source": [
    "cv = tx.CountVectorizer()\n",
    "cv.fit(corpus['text'])\n",
    "print('Всего', len(cv.vocabulary_.keys()), 'токенов')\n",
    "\n",
    "tr_xtrain = cv.transform(xtrain)\n",
    "tr_xtest = cv.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Логистическая регрессия: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.98472   0.97123   0.97793       730\n",
      "        spam    0.93333   0.96393   0.94839       305\n",
      "\n",
      "    accuracy                        0.96908      1035\n",
      "   macro avg    0.95903   0.96758   0.96316      1035\n",
      "weighted avg    0.96958   0.96908   0.96922      1035\n",
      " Случайный лес: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.97796   0.97260   0.97527       730\n",
      "        spam    0.93528   0.94754   0.94137       305\n",
      "\n",
      "    accuracy                        0.96522      1035\n",
      "   macro avg    0.95662   0.96007   0.95832      1035\n",
      "weighted avg    0.96538   0.96522   0.96528      1035\n",
      " Наивный Байес: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.98889   0.97534   0.98207       730\n",
      "        spam    0.94286   0.97377   0.95806       305\n",
      "\n",
      "    accuracy                        0.97488      1035\n",
      "   macro avg    0.96587   0.97456   0.97007      1035\n",
      "weighted avg    0.97532   0.97488   0.97500      1035\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr =  lm.LogisticRegression()\n",
    "rfc = en.RandomForestClassifier()\n",
    "cnb = nb.ComplementNB()\n",
    "\n",
    "lr.fit(tr_xtrain,ytrain)\n",
    "rfc.fit(tr_xtrain,ytrain)\n",
    "cnb.fit(tr_xtrain,ytrain)\n",
    "\n",
    "pred_lr = lr.predict(tr_xtest)\n",
    "pred_rfc = rfc.predict(tr_xtest)\n",
    "pred_cnb = cnb.predict(tr_xtest)\n",
    "\n",
    "print('Логистическая регрессия: \\n', me.classification_report(ytest, pred_lr, digits=5),\n",
    "    'Случайный лес: \\n', me.classification_report(ytest, pred_rfc, digits=5),\n",
    "    'Наивный Байес: \\n', me.classification_report(ytest, pred_cnb, digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- -- \n",
    "Найдем все стоп-слова, которые встречаются в большом количестве сообщений (более 500 , что около 9% всей выборки) и уберем их:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего 117 токенов\n",
      "{'what', 'out', 'that', 'of', 'please', 'has', 'like', 'some', 'enron', 'us', 'on', 'do', 'they', 'at', 'gas', 'see', 'if', 'daren', 'email', 'it', 'all', 'com', 'message', 'about', 'let', 'questions', 'should', 're', 'only', 'new', '01', 'and', 'can', 'for', 'have', 'nom', 'with', 'just', 'thanks', '2000', 'so', 'which', 'information', 'when', 'been', 'hou', 'need', 'ect', 'get', 'into', 'by', 'but', 'know', 'was', 'there', 'my', 'as', 'our', 'xls', 'attached', 'will', 'meter', 'time', 'mmbtu', 'pm', '03', 'one', 'subject', 'am', 'cc', 'farmer', 'would', 'we', '000', '2001', 'the', '11', 'also', 'here', 'an', 'http', 'to', 'corp', 'forwarded', 'in', 'may', 'no', 'any', '12', 'these', 'file', 'price', 'you', 'now', 'hpl', 'are', 'your', 'this', 'month', 'sent', 'be', '30', '02', 'or', 'call', 'up', 'deal', '20', 'me', '10', 'volume', 'day', 'from', '00', 'is', 'more', 'not'}\n"
     ]
    }
   ],
   "source": [
    "cv = tx.CountVectorizer(max_df = 500)\n",
    "cv.fit(corpus['text'])\n",
    "stop = cv.stop_words_\n",
    "print('Всего', len(stop), 'токенов')\n",
    "print(stop)\n",
    "\n",
    "cv = tx.CountVectorizer(analyzer = 'word', stop_words = stop)\n",
    "cv.fit(corpus['text'])\n",
    "tr_xtrain = cv.transform(xtrain)\n",
    "tr_xtest = cv.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Логистическая регрессия: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.98212   0.97808   0.98010       730\n",
      "        spam    0.94805   0.95738   0.95269       305\n",
      "\n",
      "    accuracy                        0.97198      1035\n",
      "   macro avg    0.96509   0.96773   0.96639      1035\n",
      "weighted avg    0.97208   0.97198   0.97202      1035\n",
      " Случайный лес: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.98580   0.95068   0.96792       730\n",
      "        spam    0.89124   0.96721   0.92767       305\n",
      "\n",
      "    accuracy                        0.95556      1035\n",
      "   macro avg    0.93852   0.95895   0.94780      1035\n",
      "weighted avg    0.95793   0.95556   0.95606      1035\n",
      " Наивный Байес: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.98491   0.98356   0.98424       730\n",
      "        spam    0.96078   0.96393   0.96236       305\n",
      "\n",
      "    accuracy                        0.97778      1035\n",
      "   macro avg    0.97285   0.97375   0.97330      1035\n",
      "weighted avg    0.97780   0.97778   0.97779      1035\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr =  lm.LogisticRegression()\n",
    "rfc = en.RandomForestClassifier()\n",
    "cnb = nb.ComplementNB()\n",
    "\n",
    "lr.fit(tr_xtrain,ytrain)\n",
    "rfc.fit(tr_xtrain,ytrain)\n",
    "cnb.fit(tr_xtrain,ytrain)\n",
    "\n",
    "pred_lr = lr.predict(tr_xtest)\n",
    "pred_rfc = rfc.predict(tr_xtest)\n",
    "pred_cnb = cnb.predict(tr_xtest)\n",
    "\n",
    "print('Логистическая регрессия: \\n', me.classification_report(ytest, pred_lr, digits=5),\n",
    "    'Случайный лес: \\n', me.classification_report(ytest, pred_rfc, digits=5),\n",
    "    'Наивный Байес: \\n', me.classification_report(ytest, pred_cnb, digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдем все стоп-слова, которые встречаются в маленьком количестве сообщений (менее 10, что около 0.18% всей выборки), отфильтруем только те слова, которые характерны какому-то классу (разница в частотах по классам более, чем в 7 раза):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = tx.CountVectorizer(min_df = 10)\n",
    "cv.fit(corpus['text'])\n",
    "stop = cv.stop_words_\n",
    "print('Всего', len(stop), 'токенов отмечены как редкие.')\n",
    "#print(stop) # не будем выводить список слов, так как он слишком большой\n",
    "\n",
    "all_spam = corpus[corpus['label']=='spam'].groupby('label').agg({'text':' '.join}).to_string()\n",
    "all_ham = corpus[corpus['label']=='ham'].groupby('label').agg({'text':' '.join}).to_string()\n",
    "temp = set()\n",
    "for i in stop:\n",
    "    frH = all_ham.count(i + ' ')/cntH\n",
    "    frS = all_spam.count(i + ' ')/cntS\n",
    "    if(frS != 0 and frH != 0):\n",
    "        if(not(frS/frH > 7 or frS/frH < 1/7)):\n",
    "            temp.add(i)\n",
    "stop = temp\n",
    "print('Из них {0} удовлетворяют условиям.'.format(len(stop)))\n",
    "\n",
    "cv = tx.CountVectorizer(analyzer = 'word', stop_words = stop)\n",
    "cv.fit(corpus['text'])\n",
    "tr_xtrain = cv.transform(xtrain)\n",
    "tr_xtest = cv.transform(xtest)\n",
    "print('Работаем с {0} - {1} = {2}'.format(len(cv.vocabulary_) + len(stop), len(stop), len(cv.vocabulary_)), 'токенами.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = tx.CountVectorizer(stop_words = stop)\n",
    "cv.fit(corpus['text'])\n",
    "tr_xtrain = cv.transform(xtrain)\n",
    "tr_xtest = cv.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr =  lm.LogisticRegression()\n",
    "rfc = en.RandomForestClassifier()\n",
    "cnb = nb.ComplementNB()\n",
    "\n",
    "lr.fit(tr_xtrain,ytrain)\n",
    "rfc.fit(tr_xtrain,ytrain)\n",
    "cnb.fit(tr_xtrain,ytrain)\n",
    "\n",
    "pred_lr = lr.predict(tr_xtest)\n",
    "pred_rfc = rfc.predict(tr_xtest)\n",
    "pred_cnb = cnb.predict(tr_xtest)\n",
    "\n",
    "print('Логистическая регрессия: \\n', me.classification_report(ytest, pred_lr, digits=5),\n",
    "    'Случайный лес: \\n', me.classification_report(ytest, pred_rfc, digits=5),\n",
    "    'Наивный Байес: \\n', me.classification_report(ytest, pred_cnb, digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдем все стоп-слова, которые встречаются в маленьком количестве сообщений (менее 10, что около 0.18% всей выборки), отфильтруем только те слова, которые не характерны какому-то классу (разница в частотах по классам менее, чем в 3 раза):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = tx.CountVectorizer(min_df = 10)\n",
    "cv.fit(corpus['text'])\n",
    "stop = cv.stop_words_\n",
    "print('Всего', len(stop), 'токенов отмечены как редкие.')\n",
    "#print(stop) # не будем выводить список слов, так как он слишком большой\n",
    "\n",
    "all_spam = corpus[corpus['label']=='spam'].groupby('label').agg({'text':' '.join}).to_string()\n",
    "all_ham = corpus[corpus['label']=='ham'].groupby('label').agg({'text':' '.join}).to_string()\n",
    "temp = set()\n",
    "for i in stop:\n",
    "    frH = all_ham.count(i + ' ')/cntH\n",
    "    frS = all_spam.count(i + ' ')/cntS\n",
    "    if(frS != 0 and frH != 0):\n",
    "        if(not(frS/frH < 3 or frS/frH >1/3):\n",
    "            temp.add(i)\n",
    "stop = temp\n",
    "print('Из них {0} удовлетворяют условиям.'.format(len(stop)))\n",
    "\n",
    "cv = tx.CountVectorizer(analyzer = 'word', stop_words = stop)\n",
    "cv.fit(corpus['text'])\n",
    "tr_xtrain = cv.transform(xtrain)\n",
    "tr_xtest = cv.transform(xtest)\n",
    "print('Работаем с {0} - {1} = {2}'.format(len(cv.vocabulary_) + len(stop), len(stop), len(cv.vocabulary_)), 'токенами.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = tx.CountVectorizer(stop_words = stop)\n",
    "cv.fit(corpus['text'])\n",
    "tr_xtrain = cv.transform(xtrain)\n",
    "tr_xtest = cv.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr =  lm.LogisticRegression()\n",
    "rfc = en.RandomForestClassifier()\n",
    "cnb = nb.ComplementNB()\n",
    "\n",
    "lr.fit(tr_xtrain,ytrain)\n",
    "rfc.fit(tr_xtrain,ytrain)\n",
    "cnb.fit(tr_xtrain,ytrain)\n",
    "\n",
    "pred_lr = lr.predict(tr_xtest)\n",
    "pred_rfc = rfc.predict(tr_xtest)\n",
    "pred_cnb = cnb.predict(tr_xtest)\n",
    "\n",
    "print('Логистическая регрессия: \\n', me.classification_report(ytest, pred_lr, digits=5),\n",
    "    'Случайный лес: \\n', me.classification_report(ytest, pred_rfc, digits=5),\n",
    "    'Наивный Байес: \\n', me.classification_report(ytest, pred_cnb, digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем встроенный список стоп-слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Логистическая регрессия: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.97465   1.00000   0.98716       961\n",
      "        spam    1.00000   0.83766   0.91166       154\n",
      "\n",
      "    accuracy                        0.97758      1115\n",
      "   macro avg    0.98732   0.91883   0.94941      1115\n",
      "weighted avg    0.97815   0.97758   0.97673      1115\n",
      " Случайный лес: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.97660   0.99896   0.98765       961\n",
      "        spam    0.99242   0.85065   0.91608       154\n",
      "\n",
      "    accuracy                        0.97848      1115\n",
      "   macro avg    0.98451   0.92480   0.95187      1115\n",
      "weighted avg    0.97879   0.97848   0.97777      1115\n",
      " Наивный Байес: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.99552   0.92508   0.95901       961\n",
      "        spam    0.67568   0.97403   0.79787       154\n",
      "\n",
      "    accuracy                        0.93184      1115\n",
      "   macro avg    0.83560   0.94955   0.87844      1115\n",
      "weighted avg    0.95134   0.93184   0.93675      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv = tx.CountVectorizer(stop_words = 'english')\n",
    "cv.fit(corpus['text'])\n",
    "tr_xtrain = cv.transform(xtrain)\n",
    "tr_xtest = cv.transform(xtest)\n",
    "\n",
    "lr =  lm.LogisticRegression()\n",
    "rfc = en.RandomForestClassifier()\n",
    "cnb = nb.ComplementNB()\n",
    "\n",
    "lr.fit(tr_xtrain,ytrain)\n",
    "rfc.fit(tr_xtrain,ytrain)\n",
    "cnb.fit(tr_xtrain,ytrain)\n",
    "\n",
    "pred_lr = lr.predict(tr_xtest)\n",
    "pred_rfc = rfc.predict(tr_xtest)\n",
    "pred_cnb = cnb.predict(tr_xtest)\n",
    "\n",
    "print('Логистическая регрессия: \\n', me.classification_report(ytest, pred_lr, digits=5),\n",
    "    'Случайный лес: \\n', me.classification_report(ytest, pred_rfc, digits=5),\n",
    "    'Наивный Байес: \\n', me.classification_report(ytest, pred_cnb, digits=5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "b2a2d0a56c23e8ff240e12265c41bd4cf4ebc4a5bdfa5544f52816f127301f16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
