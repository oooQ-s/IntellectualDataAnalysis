{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Байесовская классификация спам/не спам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn.feature_extraction.text as tx, sklearn.naive_bayes as nb, sklearn.linear_model as lm, sklearn.ensemble as en, sklearn.metrics as me, sklearn.model_selection as ms\n",
    "from warnings import filterwarnings as fw\n",
    "fw('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Узнаем вариант"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('А')%3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прочитаем корпус текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>605</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: enron methanol ; meter # : 988291\\r\\n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2349</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: hpl nom for january 9 , 2001\\r\\n( see...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3624</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: neon retreat\\r\\nho ho ho , we ' re ar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4685</td>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: photoshop , windows , office . cheap ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2030</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: re : indian springs\\r\\nthis deal is t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5166</th>\n",
       "      <td>1518</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: put the 10 on the ft\\r\\nthe transport...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5167</th>\n",
       "      <td>404</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: 3 / 4 / 2000 and following noms\\r\\nhp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5168</th>\n",
       "      <td>2933</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: calpine daily gas nomination\\r\\n&gt;\\r\\n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5169</th>\n",
       "      <td>1409</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: industrial worksheets for august 2000...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5170</th>\n",
       "      <td>4807</td>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: important online banking alert\\r\\ndea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5171 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0 label                                               text  \\\n",
       "0            605   ham  Subject: enron methanol ; meter # : 988291\\r\\n...   \n",
       "1           2349   ham  Subject: hpl nom for january 9 , 2001\\r\\n( see...   \n",
       "2           3624   ham  Subject: neon retreat\\r\\nho ho ho , we ' re ar...   \n",
       "3           4685  spam  Subject: photoshop , windows , office . cheap ...   \n",
       "4           2030   ham  Subject: re : indian springs\\r\\nthis deal is t...   \n",
       "...          ...   ...                                                ...   \n",
       "5166        1518   ham  Subject: put the 10 on the ft\\r\\nthe transport...   \n",
       "5167         404   ham  Subject: 3 / 4 / 2000 and following noms\\r\\nhp...   \n",
       "5168        2933   ham  Subject: calpine daily gas nomination\\r\\n>\\r\\n...   \n",
       "5169        1409   ham  Subject: industrial worksheets for august 2000...   \n",
       "5170        4807  spam  Subject: important online banking alert\\r\\ndea...   \n",
       "\n",
       "      label_num  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             1  \n",
       "4             0  \n",
       "...         ...  \n",
       "5166          0  \n",
       "5167          0  \n",
       "5168          0  \n",
       "5169          0  \n",
       "5170          1  \n",
       "\n",
       "[5171 rows x 4 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pd.read_csv('Emailspam2.csv')\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проанализируем количество слов по классам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего строк: 5171 \n",
      "Из них 1499 spam и 3672 ham \n",
      "Всего слов:  1083244 \n",
      "из них 320863 spam и 762381 ham\n"
     ]
    }
   ],
   "source": [
    "cnt, cntS, cntH = 0, 0, 0\n",
    "for i in corpus['text']:\n",
    "    cnt += len(i.split(' '))\n",
    "for i in corpus[corpus['label']=='spam']['text']:\n",
    "    cntS += len(i.split(' '))\n",
    "for i in corpus[corpus['label']=='ham']['text']:\n",
    "    cntH += len(i.split(' '))\n",
    "print('Всего строк:', corpus.shape[0], '\\nИз них', \n",
    "    corpus['label'].value_counts()['spam'], 'spam и',corpus['label'].value_counts()['ham'], 'ham', \n",
    "    '\\nВсего слов: ', cnt, '\\nиз них', cntS, 'spam и', cntH, 'ham')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучающая выборка: (4136,) \n",
      "Тестовая выборка (1035,)\n"
     ]
    }
   ],
   "source": [
    "xtrain, xtest, ytrain, ytest = ms.train_test_split(corpus['text'],corpus['label'], test_size=0.2)\n",
    "print('Обучающая выборка:', xtrain.shape,'\\nТестовая выборка', xtest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- -- \n",
    "Используем все слова с учетом регистра:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего 50448 токенов\n"
     ]
    }
   ],
   "source": [
    "cv = tx.CountVectorizer(lowercase=False)\n",
    "cv.fit(corpus['text'])\n",
    "print('Всего', len(cv.vocabulary_.keys()), 'токенов')\n",
    "\n",
    "tr_xtrain = cv.transform(xtrain)\n",
    "tr_xtest = cv.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Логистическая регрессия: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.99309   0.97823   0.98561       735\n",
      "        spam    0.94855   0.98333   0.96563       300\n",
      "\n",
      "    accuracy                        0.97971      1035\n",
      "   macro avg    0.97082   0.98078   0.97562      1035\n",
      "weighted avg    0.98018   0.97971   0.97982      1035\n",
      " Случайный лес: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.96929   0.98776   0.97844       735\n",
      "        spam    0.96853   0.92333   0.94539       300\n",
      "\n",
      "    accuracy                        0.96908      1035\n",
      "   macro avg    0.96891   0.95554   0.96191      1035\n",
      "weighted avg    0.96907   0.96908   0.96886      1035\n",
      " Наивный Байес: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.98909   0.98639   0.98774       735\n",
      "        spam    0.96689   0.97333   0.97010       300\n",
      "\n",
      "    accuracy                        0.98261      1035\n",
      "   macro avg    0.97799   0.97986   0.97892      1035\n",
      "weighted avg    0.98265   0.98261   0.98263      1035\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr =  lm.LogisticRegression()\n",
    "rfc = en.RandomForestClassifier()\n",
    "cnb = nb.ComplementNB()\n",
    "\n",
    "lr.fit(tr_xtrain,ytrain)\n",
    "rfc.fit(tr_xtrain,ytrain)\n",
    "cnb.fit(tr_xtrain,ytrain)\n",
    "\n",
    "pred_lr = lr.predict(tr_xtest)\n",
    "pred_rfc = rfc.predict(tr_xtest)\n",
    "pred_cnb = cnb.predict(tr_xtest)\n",
    "\n",
    "print('Логистическая регрессия: \\n', me.classification_report(ytest, pred_lr, digits=5),\n",
    "    'Случайный лес: \\n', me.classification_report(ytest, pred_rfc, digits=5),\n",
    "    'Наивный Байес: \\n', me.classification_report(ytest, pred_cnb, digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- -- \n",
    "Используем все слова, но в одном регистре:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего 50447 токенов\n"
     ]
    }
   ],
   "source": [
    "cv = tx.CountVectorizer()\n",
    "cv.fit(corpus['text'])\n",
    "print('Всего', len(cv.vocabulary_.keys()), 'токенов')\n",
    "\n",
    "tr_xtrain = cv.transform(xtrain)\n",
    "tr_xtest = cv.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Логистическая регрессия: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.99309   0.97823   0.98561       735\n",
      "        spam    0.94855   0.98333   0.96563       300\n",
      "\n",
      "    accuracy                        0.97971      1035\n",
      "   macro avg    0.97082   0.98078   0.97562      1035\n",
      "weighted avg    0.98018   0.97971   0.97982      1035\n",
      " Случайный лес: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.97323   0.98912   0.98111       735\n",
      "        spam    0.97222   0.93333   0.95238       300\n",
      "\n",
      "    accuracy                        0.97295      1035\n",
      "   macro avg    0.97272   0.96122   0.96674      1035\n",
      "weighted avg    0.97294   0.97295   0.97278      1035\n",
      " Наивный Байес: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.98910   0.98776   0.98843       735\n",
      "        spam    0.97010   0.97333   0.97171       300\n",
      "\n",
      "    accuracy                        0.98357      1035\n",
      "   macro avg    0.97960   0.98054   0.98007      1035\n",
      "weighted avg    0.98359   0.98357   0.98358      1035\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr =  lm.LogisticRegression()\n",
    "rfc = en.RandomForestClassifier()\n",
    "cnb = nb.ComplementNB()\n",
    "\n",
    "lr.fit(tr_xtrain,ytrain)\n",
    "rfc.fit(tr_xtrain,ytrain)\n",
    "cnb.fit(tr_xtrain,ytrain)\n",
    "\n",
    "pred_lr = lr.predict(tr_xtest)\n",
    "pred_rfc = rfc.predict(tr_xtest)\n",
    "pred_cnb = cnb.predict(tr_xtest)\n",
    "\n",
    "print('Логистическая регрессия: \\n', me.classification_report(ytest, pred_lr, digits=5),\n",
    "    'Случайный лес: \\n', me.classification_report(ytest, pred_rfc, digits=5),\n",
    "    'Наивный Байес: \\n', me.classification_report(ytest, pred_cnb, digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- -- \n",
    "Найдем все стоп-слова, которые встречаются в большом количестве сообщений (более 500 , что около 9% всей выборки) и уберем их:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего 117 токенов\n",
      "{'would', 'this', 'from', 'message', 'some', 'here', 'by', 'to', 'forwarded', 'are', 'was', 'into', 'the', 'month', 'that', 'no', 'know', 'subject', '11', 'information', 'about', 'new', 'am', '03', 'enron', 'do', '000', 'you', 'mmbtu', '20', 'out', 'been', 'please', '02', 'and', 'get', 'http', 'or', 'be', 'have', 'farmer', '2000', 'corp', 'one', 'has', 'ect', 'can', 'it', 'my', 'see', 'of', 'price', 'up', 'in', '00', 'need', 'will', 'xls', 're', 'let', '10', 'just', 'attached', 'sent', '30', 'call', 'like', 'us', 'me', 'hpl', 'we', 'as', 'daren', 'time', 'if', 'gas', 'at', 'our', 'volume', 'is', 'on', 'not', 'day', 'more', '12', 'which', 'file', 'com', 'pm', '2001', 'email', 'your', 'nom', 'meter', 'thanks', 'they', 'for', 'only', 'all', 'there', 'what', 'should', 'any', 'an', 'hou', 'so', 'questions', 'cc', 'deal', '01', 'may', 'when', 'but', 'now', 'with', 'these', 'also'}\n"
     ]
    }
   ],
   "source": [
    "cv = tx.CountVectorizer(max_df = 500)\n",
    "cv.fit(corpus['text'])\n",
    "stop = cv.stop_words_\n",
    "print('Всего', len(stop), 'токенов')\n",
    "print(stop)\n",
    "\n",
    "cv = tx.CountVectorizer(analyzer = 'word', stop_words = stop)\n",
    "cv.fit(corpus['text'])\n",
    "tr_xtrain = cv.transform(xtrain)\n",
    "tr_xtest = cv.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Логистическая регрессия: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.97844   0.98776   0.98307       735\n",
      "        spam    0.96928   0.94667   0.95784       300\n",
      "\n",
      "    accuracy                        0.97585      1035\n",
      "   macro avg    0.97386   0.96721   0.97046      1035\n",
      "weighted avg    0.97578   0.97585   0.97576      1035\n",
      " Случайный лес: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.98476   0.96735   0.97598       735\n",
      "        spam    0.92332   0.96333   0.94290       300\n",
      "\n",
      "    accuracy                        0.96618      1035\n",
      "   macro avg    0.95404   0.96534   0.95944      1035\n",
      "weighted avg    0.96696   0.96618   0.96639      1035\n",
      " Наивный Байес: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.98372   0.98639   0.98505       735\n",
      "        spam    0.96644   0.96000   0.96321       300\n",
      "\n",
      "    accuracy                        0.97874      1035\n",
      "   macro avg    0.97508   0.97320   0.97413      1035\n",
      "weighted avg    0.97871   0.97874   0.97872      1035\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr =  lm.LogisticRegression()\n",
    "rfc = en.RandomForestClassifier()\n",
    "cnb = nb.ComplementNB()\n",
    "\n",
    "lr.fit(tr_xtrain,ytrain)\n",
    "rfc.fit(tr_xtrain,ytrain)\n",
    "cnb.fit(tr_xtrain,ytrain)\n",
    "\n",
    "pred_lr = lr.predict(tr_xtest)\n",
    "pred_rfc = rfc.predict(tr_xtest)\n",
    "pred_cnb = cnb.predict(tr_xtest)\n",
    "\n",
    "print('Логистическая регрессия: \\n', me.classification_report(ytest, pred_lr, digits=5),\n",
    "    'Случайный лес: \\n', me.classification_report(ytest, pred_rfc, digits=5),\n",
    "    'Наивный Байес: \\n', me.classification_report(ytest, pred_cnb, digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдем все стоп-слова, которые встречаются в маленьком количестве сообщений (менее 10, что около 0.18% всей выборки), отфильтруем только те слова, которые характерны какому-то классу (разница в частотах по классам более, чем в 7 раза):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего 45591 токенов отмечены как редкие.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\sixxio\\Desktop\\ds\\IntellectualDataAnalysis-1\\task 5 - spam artem\\lab5_Nazarov.ipynb Ячейка 21\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sixxio/Desktop/ds/IntellectualDataAnalysis-1/task%205%20-%20spam%20artem/lab5_Nazarov.ipynb#X26sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m temp \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sixxio/Desktop/ds/IntellectualDataAnalysis-1/task%205%20-%20spam%20artem/lab5_Nazarov.ipynb#X26sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m stop:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/sixxio/Desktop/ds/IntellectualDataAnalysis-1/task%205%20-%20spam%20artem/lab5_Nazarov.ipynb#X26sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     frH \u001b[39m=\u001b[39m all_ham\u001b[39m.\u001b[39mcount(i \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)\u001b[39m/\u001b[39mcntH\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sixxio/Desktop/ds/IntellectualDataAnalysis-1/task%205%20-%20spam%20artem/lab5_Nazarov.ipynb#X26sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     frS \u001b[39m=\u001b[39m all_spam\u001b[39m.\u001b[39mcount(i \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)\u001b[39m/\u001b[39mcntS\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sixxio/Desktop/ds/IntellectualDataAnalysis-1/task%205%20-%20spam%20artem/lab5_Nazarov.ipynb#X26sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mif\u001b[39;00m(frS \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m frH \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cv = tx.CountVectorizer(min_df = 10)\n",
    "cv.fit(corpus['text'])\n",
    "stop = cv.stop_words_\n",
    "print('Всего', len(stop), 'токенов отмечены как редкие.')\n",
    "#print(stop) # не будем выводить список слов, так как он слишком большой\n",
    "\n",
    "all_spam = corpus[corpus['label']=='spam'].groupby('label').agg({'text':' '.join}).to_string()\n",
    "all_ham = corpus[corpus['label']=='ham'].groupby('label').agg({'text':' '.join}).to_string()\n",
    "temp = set()\n",
    "for i in stop:\n",
    "    frH = all_ham.count(i + ' ')/cntH\n",
    "    frS = all_spam.count(i + ' ')/cntS\n",
    "    if(frS != 0 and frH != 0):\n",
    "        if(not(frS/frH > 7 or frS/frH < 1/7)):\n",
    "            temp.add(i)\n",
    "stop = temp\n",
    "print('Из них {0} удовлетворяют условиям.'.format(len(stop)))\n",
    "\n",
    "cv = tx.CountVectorizer(analyzer = 'word', stop_words = stop)\n",
    "cv.fit(corpus['text'])\n",
    "tr_xtrain = cv.transform(xtrain)\n",
    "tr_xtest = cv.transform(xtest)\n",
    "print('Работаем с {0} - {1} = {2}'.format(len(cv.vocabulary_) + len(stop), len(stop), len(cv.vocabulary_)), 'токенами.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Логистическая регрессия: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.98893   0.97147   0.98012       736\n",
      "        spam    0.93269   0.97324   0.95254       299\n",
      "\n",
      "    accuracy                        0.97198      1035\n",
      "   macro avg    0.96081   0.97236   0.96633      1035\n",
      "weighted avg    0.97269   0.97198   0.97215      1035\n",
      " Случайный лес: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.96925   0.98505   0.97709       736\n",
      "        spam    0.96167   0.92308   0.94198       299\n",
      "\n",
      "    accuracy                        0.96715      1035\n",
      "   macro avg    0.96546   0.95407   0.95953      1035\n",
      "weighted avg    0.96706   0.96715   0.96695      1035\n",
      " Наивный Байес: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.99186   0.99321   0.99253       736\n",
      "        spam    0.98322   0.97993   0.98157       299\n",
      "\n",
      "    accuracy                        0.98937      1035\n",
      "   macro avg    0.98754   0.98657   0.98705      1035\n",
      "weighted avg    0.98936   0.98937   0.98937      1035\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr =  lm.LogisticRegression()\n",
    "rfc = en.RandomForestClassifier()\n",
    "cnb = nb.ComplementNB()\n",
    "\n",
    "lr.fit(tr_xtrain,ytrain)\n",
    "rfc.fit(tr_xtrain,ytrain)\n",
    "cnb.fit(tr_xtrain,ytrain)\n",
    "\n",
    "pred_lr = lr.predict(tr_xtest)\n",
    "pred_rfc = rfc.predict(tr_xtest)\n",
    "pred_cnb = cnb.predict(tr_xtest)\n",
    "\n",
    "print('Логистическая регрессия: \\n', me.classification_report(ytest, pred_lr, digits=5),\n",
    "    'Случайный лес: \\n', me.classification_report(ytest, pred_rfc, digits=5),\n",
    "    'Наивный Байес: \\n', me.classification_report(ytest, pred_cnb, digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем встроенный список стоп-слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Логистическая регрессия: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.99309   0.97823   0.98561       735\n",
      "        spam    0.94855   0.98333   0.96563       300\n",
      "\n",
      "    accuracy                        0.97971      1035\n",
      "   macro avg    0.97082   0.98078   0.97562      1035\n",
      "weighted avg    0.98018   0.97971   0.97982      1035\n",
      " Случайный лес: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.97718   0.99048   0.98378       735\n",
      "        spam    0.97586   0.94333   0.95932       300\n",
      "\n",
      "    accuracy                        0.97681      1035\n",
      "   macro avg    0.97652   0.96690   0.97155      1035\n",
      "weighted avg    0.97680   0.97681   0.97669      1035\n",
      " Наивный Байес: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.98910   0.98776   0.98843       735\n",
      "        spam    0.97010   0.97333   0.97171       300\n",
      "\n",
      "    accuracy                        0.98357      1035\n",
      "   macro avg    0.97960   0.98054   0.98007      1035\n",
      "weighted avg    0.98359   0.98357   0.98358      1035\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv = tx.CountVectorizer()\n",
    "cv.fit(corpus['text'])\n",
    "tr_xtrain = cv.transform(xtrain)\n",
    "tr_xtest = cv.transform(xtest)\n",
    "\n",
    "lr =  lm.LogisticRegression()\n",
    "rfc = en.RandomForestClassifier()\n",
    "cnb = nb.ComplementNB()\n",
    "\n",
    "lr.fit(tr_xtrain,ytrain)\n",
    "rfc.fit(tr_xtrain,ytrain)\n",
    "cnb.fit(tr_xtrain,ytrain)\n",
    "\n",
    "pred_lr = lr.predict(tr_xtest)\n",
    "pred_rfc = rfc.predict(tr_xtest)\n",
    "pred_cnb = cnb.predict(tr_xtest)\n",
    "\n",
    "print('Логистическая регрессия: \\n', me.classification_report(ytest, pred_lr, digits=5),\n",
    "    'Случайный лес: \\n', me.classification_report(ytest, pred_rfc, digits=5),\n",
    "    'Наивный Байес: \\n', me.classification_report(ytest, pred_cnb, digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "количество слов: 1 score: 0.7101449275362319\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.71014   1.00000   0.83051       735\n",
      "        spam    0.00000   0.00000   0.00000       300\n",
      "\n",
      "    accuracy                        0.71014      1035\n",
      "   macro avg    0.35507   0.50000   0.41525      1035\n",
      "weighted avg    0.50431   0.71014   0.58978      1035\n",
      "\n",
      "количество слов: 10 score: 0.7101449275362319\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.71014   1.00000   0.83051       735\n",
      "        spam    0.00000   0.00000   0.00000       300\n",
      "\n",
      "    accuracy                        0.71014      1035\n",
      "   macro avg    0.35507   0.50000   0.41525      1035\n",
      "weighted avg    0.50431   0.71014   0.58978      1035\n",
      "\n",
      "количество слов: 100 score: 0.5690821256038647\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham    1.00000   0.39320   0.56445       735\n",
      "        spam    0.40214   1.00000   0.57361       300\n",
      "\n",
      "    accuracy                        0.56908      1035\n",
      "   macro avg    0.70107   0.69660   0.56903      1035\n",
      "weighted avg    0.82671   0.56908   0.56711      1035\n",
      "\n",
      "количество слов: 1000 score: 0.8048309178743961\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.85486   0.87347   0.86406       735\n",
      "        spam    0.67254   0.63667   0.65411       300\n",
      "\n",
      "    accuracy                        0.80483      1035\n",
      "   macro avg    0.76370   0.75507   0.75909      1035\n",
      "weighted avg    0.80201   0.80483   0.80321      1035\n",
      "\n",
      "количество слов: 10000 score: 0.9439613526570049\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.94716   0.97551   0.96113       735\n",
      "        spam    0.93525   0.86667   0.89965       300\n",
      "\n",
      "    accuracy                        0.94396      1035\n",
      "   macro avg    0.94121   0.92109   0.93039      1035\n",
      "weighted avg    0.94371   0.94396   0.94331      1035\n",
      "\n",
      "количество слов: 50447 score: 0.9835748792270531\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.98910   0.98776   0.98843       735\n",
      "        spam    0.97010   0.97333   0.97171       300\n",
      "\n",
      "    accuracy                        0.98357      1035\n",
      "   macro avg    0.97960   0.98054   0.98007      1035\n",
      "weighted avg    0.98359   0.98357   0.98358      1035\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    cv = tx.CountVectorizer()\n",
    "    cv.fit(corpus['text'])\n",
    "    tr_xtrain = cv.transform(xtrain)\n",
    "    tr_xtest = cv.transform(xtest)\n",
    "    rfc = en.RandomForestClassifier()\n",
    "    rfc.fit(tr_xtrain,ytrain)\n",
    "\n",
    "    imp = pd.DataFrame({'name' : cv.vocabulary_.keys(), 'importance' : rfc.feature_importances_}) \n",
    "    imp = imp.sort_values(by = 'importance', ascending = False)\n",
    "    top100 = imp.head(n=10**i)['name'].to_numpy()\n",
    "\n",
    "    cv = tx.CountVectorizer(vocabulary = top100)\n",
    "    cv.fit(corpus['text'])\n",
    "    tr_xtrain = cv.transform(xtrain)\n",
    "    tr_xtest = cv.transform(xtest)\n",
    "\n",
    "    lr =  lm.LogisticRegression()\n",
    "    rfc = en.RandomForestClassifier()\n",
    "    cnb = nb.ComplementNB()\n",
    "\n",
    "    lr.fit(tr_xtrain,ytrain)\n",
    "    rfc.fit(tr_xtrain,ytrain)\n",
    "    cnb.fit(tr_xtrain,ytrain)\n",
    "\n",
    "    pred_lr = lr.predict(tr_xtest)\n",
    "    pred_rfc = rfc.predict(tr_xtest)\n",
    "    pred_cnb = cnb.predict(tr_xtest)\n",
    "\n",
    "    print('количество слов:', len(top100), 'score:', cnb.score(tr_xtest,ytest))\n",
    "    print(me.classification_report(ytest,pred_cnb,digits=5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b2a2d0a56c23e8ff240e12265c41bd4cf4ebc4a5bdfa5544f52816f127301f16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
