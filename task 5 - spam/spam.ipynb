{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn.feature_extraction.text as tx, sklearn.naive_bayes as nb, sklearn.linear_model as lm, sklearn.ensemble as en, sklearn.metrics as me, sklearn.model_selection as ms\n",
    "from warnings import filterwarnings as fw\n",
    "fw('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "все слова с учетом регистра<br>\n",
    "все в одном регистре<br>\n",
    "без стоп слов<br>\n",
    "выделение специфичных для классов<br>\n",
    "-- -- \n",
    "лог регрессия, случайный лес, байес"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('Н')%3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitpredict(xtrain,ytrain,xtest,ytest):\n",
    "    \n",
    "    lr =  lm.LogisticRegression()\n",
    "    rfc = en.RandomForestClassifier()\n",
    "    cnb = nb.ComplementNB()\n",
    "\n",
    "    lr.fit(xtrain,ytrain)\n",
    "    rfc.fit(xtrain,ytrain)\n",
    "    cnb.fit(xtrain,ytrain)\n",
    "\n",
    "    pred_lr = lr.predict(xtest)\n",
    "    pred_rfc = rfc.predict(xtest)\n",
    "    pred_cnb = cnb.predict(xtest)\n",
    "    \n",
    "    print('Логистическая регрессия: \\n', me.classification_report(ytest, pred_lr, digits=5),\n",
    "        'Случайный лес: \\n', me.classification_report(ytest, pred_rfc, digits=5),\n",
    "        'Наивный Байес: \\n', me.classification_report(ytest, pred_cnb, digits=5))\n",
    "    \n",
    "    return lr, rfc, cnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_csv('SMSpam.csv')\n",
    "corpus['Message'] = corpus['Message'].str.replace(r'[^\\w\\s]+','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего строк: 5572 \n",
      "Из них 747 spam и 4825 ham \n",
      "Всего слов:  87265 \n",
      "из них 17788 spam и 69477 ham\n"
     ]
    }
   ],
   "source": [
    "cnt, cntS, cntH = 0, 0, 0\n",
    "for i in corpus['Message']:\n",
    "    cnt += len(i.split(' '))\n",
    "for i in corpus[corpus['Category']=='spam']['Message']:\n",
    "    cntS += len(i.split(' '))\n",
    "for i in corpus[corpus['Category']=='ham']['Message']:\n",
    "    cntH += len(i.split(' '))\n",
    "print('Всего строк:', corpus.shape[0], '\\nИз них', \n",
    "    corpus['Category'].value_counts()['spam'], 'spam и',corpus['Category'].value_counts()['ham'], 'ham', \n",
    "    '\\nВсего слов: ', cnt, '\\nиз них', cntS, 'spam и', cntH, 'ham')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучающая выборка: (4457,) \n",
      "Тестовая выборка (1115,)\n"
     ]
    }
   ],
   "source": [
    "xtrain, xtest, ytrain, ytest = ms.train_test_split(corpus['Message'],corpus['Category'], test_size=0.2)\n",
    "print('Обучающая выборка:', xtrain.shape,'\\nТестовая выборка', xtest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- -- \n",
    "Используем все слова с учетом регистра:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего 11596 токенов\n"
     ]
    }
   ],
   "source": [
    "cv = tx.CountVectorizer(lowercase=False)\n",
    "cv.fit(corpus['Message'])\n",
    "print('Всего', len(cv.vocabulary_.keys()), 'токенов')\n",
    "\n",
    "tr_xtrain = cv.transform(xtrain)\n",
    "tr_xtest = cv.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Логистическая регрессия: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.97862   0.99896   0.98868       962\n",
      "        spam    0.99248   0.86275   0.92308       153\n",
      "\n",
      "    accuracy                        0.98027      1115\n",
      "   macro avg    0.98555   0.93085   0.95588      1115\n",
      "weighted avg    0.98052   0.98027   0.97968      1115\n",
      " Случайный лес: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.97172   1.00000   0.98566       962\n",
      "        spam    1.00000   0.81699   0.89928       153\n",
      "\n",
      "    accuracy                        0.97489      1115\n",
      "   macro avg    0.98586   0.90850   0.94247      1115\n",
      "weighted avg    0.97560   0.97489   0.97380      1115\n",
      " Наивный Байес: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.99457   0.95218   0.97292       962\n",
      "        spam    0.76289   0.96732   0.85303       153\n",
      "\n",
      "    accuracy                        0.95426      1115\n",
      "   macro avg    0.87873   0.95975   0.91297      1115\n",
      "weighted avg    0.96278   0.95426   0.95646      1115\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(), RandomForestClassifier(), ComplementNB())"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitpredict(tr_xtrain, ytrain, tr_xtest, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- -- \n",
    "Используем все слова, но в одном регистре:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего 9542 токенов\n"
     ]
    }
   ],
   "source": [
    "cv = tx.CountVectorizer()\n",
    "cv.fit(corpus['Message'])\n",
    "print('Всего', len(cv.vocabulary_.keys()), 'токенов')\n",
    "\n",
    "tr_xtrain = cv.transform(xtrain)\n",
    "tr_xtest = cv.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Логистическая регрессия: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.97864   1.00000   0.98920       962\n",
      "        spam    1.00000   0.86275   0.92632       153\n",
      "\n",
      "    accuracy                        0.98117      1115\n",
      "   macro avg    0.98932   0.93137   0.95776      1115\n",
      "weighted avg    0.98157   0.98117   0.98057      1115\n",
      " Случайный лес: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.96976   1.00000   0.98465       962\n",
      "        spam    1.00000   0.80392   0.89130       153\n",
      "\n",
      "    accuracy                        0.97309      1115\n",
      "   macro avg    0.98488   0.90196   0.93798      1115\n",
      "weighted avg    0.97391   0.97309   0.97184      1115\n",
      " Наивный Байес: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.99041   0.96570   0.97789       962\n",
      "        spam    0.81356   0.94118   0.87273       153\n",
      "\n",
      "    accuracy                        0.96233      1115\n",
      "   macro avg    0.90198   0.95344   0.92531      1115\n",
      "weighted avg    0.96614   0.96233   0.96346      1115\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(), RandomForestClassifier(), ComplementNB())"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitpredict(tr_xtrain, ytrain, tr_xtest, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- -- \n",
    "Найдем все стоп-слова, которые встречаются в большом количестве сообщений (более 500 , что около 9% всей выборки) и уберем их:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего 14 токенов\n",
      "{'to', 'me', 'is', 'of', 'for', 'have', 'and', 'it', 'the', 'your', 'you', 'call', 'my', 'in'}\n"
     ]
    }
   ],
   "source": [
    "cv = tx.CountVectorizer(max_df = 500)\n",
    "cv.fit(corpus['Message'])\n",
    "stop = cv.stop_words_\n",
    "print('Всего', len(stop), 'токенов')\n",
    "print(stop)\n",
    "\n",
    "cv = tx.CountVectorizer(analyzer = 'word', stop_words = stop)\n",
    "cv.fit(corpus['Message'])\n",
    "tr_xtrain = cv.transform(xtrain)\n",
    "tr_xtest = cv.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Логистическая регрессия: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.97566   1.00000   0.98768       962\n",
      "        spam    1.00000   0.84314   0.91489       153\n",
      "\n",
      "    accuracy                        0.97848      1115\n",
      "   macro avg    0.98783   0.92157   0.95129      1115\n",
      "weighted avg    0.97900   0.97848   0.97769      1115\n",
      " Случайный лес: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.97074   1.00000   0.98515       962\n",
      "        spam    1.00000   0.81046   0.89531       153\n",
      "\n",
      "    accuracy                        0.97399      1115\n",
      "   macro avg    0.98537   0.90523   0.94023      1115\n",
      "weighted avg    0.97475   0.97399   0.97282      1115\n",
      " Наивный Байес: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.99245   0.95634   0.97406       962\n",
      "        spam    0.77660   0.95425   0.85630       153\n",
      "\n",
      "    accuracy                        0.95605      1115\n",
      "   macro avg    0.88452   0.95529   0.91518      1115\n",
      "weighted avg    0.96283   0.95605   0.95790      1115\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(), RandomForestClassifier(), ComplementNB())"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitpredict(tr_xtrain, ytrain, tr_xtest, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдем все стоп-слова, которые встречаются в маленьком количестве сообщений (менее 10, что около 0.18% всей выборки) и уберем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего 8562 токенов\n"
     ]
    }
   ],
   "source": [
    "cv = tx.CountVectorizer(min_df = 10)\n",
    "cv.fit(corpus['Message'])\n",
    "stop = cv.stop_words_\n",
    "print('Всего', len(stop), 'токенов')\n",
    "#print(stop) // не будем выводить список слов, так как он слишком большой\n",
    "\n",
    "cv = tx.CountVectorizer(analyzer = 'word', stop_words = stop)\n",
    "cv.fit(corpus['Message'])\n",
    "tr_xtrain = cv.transform(xtrain)\n",
    "tr_xtest = cv.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Логистическая регрессия: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.98063   1.00000   0.99022       962\n",
      "        spam    1.00000   0.87582   0.93380       153\n",
      "\n",
      "    accuracy                        0.98296      1115\n",
      "   macro avg    0.99032   0.93791   0.96201      1115\n",
      "weighted avg    0.98329   0.98296   0.98248      1115\n",
      " Случайный лес: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.97467   1.00000   0.98717       962\n",
      "        spam    1.00000   0.83660   0.91103       153\n",
      "\n",
      "    accuracy                        0.97758      1115\n",
      "   macro avg    0.98734   0.91830   0.94910      1115\n",
      "weighted avg    0.97815   0.97758   0.97672      1115\n",
      " Наивный Байес: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.98934   0.96466   0.97684       962\n",
      "        spam    0.80791   0.93464   0.86667       153\n",
      "\n",
      "    accuracy                        0.96054      1115\n",
      "   macro avg    0.89862   0.94965   0.92175      1115\n",
      "weighted avg    0.96444   0.96054   0.96172      1115\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(), RandomForestClassifier(), ComplementNB())"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitpredict(tr_xtrain, ytrain, tr_xtest, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем встроенный список стоп-слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Логистическая регрессия: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.97764   1.00000   0.98869       962\n",
      "        spam    1.00000   0.85621   0.92254       153\n",
      "\n",
      "    accuracy                        0.98027      1115\n",
      "   macro avg    0.98882   0.92810   0.95561      1115\n",
      "weighted avg    0.98071   0.98027   0.97962      1115\n",
      " Случайный лес: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.96878   1.00000   0.98414       962\n",
      "        spam    1.00000   0.79739   0.88727       153\n",
      "\n",
      "    accuracy                        0.97220      1115\n",
      "   macro avg    0.98439   0.89869   0.93571      1115\n",
      "weighted avg    0.97307   0.97220   0.97085      1115\n",
      " Наивный Байес: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.99549   0.91788   0.95511       962\n",
      "        spam    0.65351   0.97386   0.78215       153\n",
      "\n",
      "    accuracy                        0.92556      1115\n",
      "   macro avg    0.82450   0.94587   0.86863      1115\n",
      "weighted avg    0.94856   0.92556   0.93138      1115\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(), RandomForestClassifier(), ComplementNB())"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = tx.CountVectorizer(stop_words = 'english')\n",
    "cv.fit(corpus['Message'])\n",
    "tr_xtrain = cv.transform(xtrain)\n",
    "tr_xtest = cv.transform(xtest)\n",
    "fitpredict(tr_xtrain, ytrain, tr_xtest, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- -- \n",
    "Используем только слова с высокой важностью:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9748878923766816"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = tx.CountVectorizer()\n",
    "cv.fit(corpus['Message'])\n",
    "tr_xtrain = cv.transform(xtrain)\n",
    "tr_xtest = cv.transform(xtest)\n",
    "\n",
    "e = en.RandomForestClassifier()\n",
    "e.fit(tr_xtrain,ytrain)\n",
    "e.score(tr_xtest,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Логистическая регрессия: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.93281   0.99584   0.96330       962\n",
      "        spam    0.95455   0.54902   0.69710       153\n",
      "\n",
      "    accuracy                        0.93453      1115\n",
      "   macro avg    0.94368   0.77243   0.83020      1115\n",
      "weighted avg    0.93580   0.93453   0.92677      1115\n",
      " Случайный лес: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.95972   0.99064   0.97494       962\n",
      "        spam    0.92623   0.73856   0.82182       153\n",
      "\n",
      "    accuracy                        0.95605      1115\n",
      "   macro avg    0.94297   0.86460   0.89838      1115\n",
      "weighted avg    0.95512   0.95605   0.95393      1115\n",
      " Наивный Байес: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.97098   0.90437   0.93649       962\n",
      "        spam    0.57991   0.83007   0.68280       153\n",
      "\n",
      "    accuracy                        0.89417      1115\n",
      "   macro avg    0.77545   0.86722   0.80964      1115\n",
      "weighted avg    0.91732   0.89417   0.90168      1115\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(), RandomForestClassifier(), ComplementNB())"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp = pd.DataFrame({'name' : cv.vocabulary_.keys(), 'importance' : e.feature_importances_}) \n",
    "imp = imp.sort_values(by = 'importance', ascending = False)\n",
    "top100 = imp.head(n=1000)['name'].to_numpy()\n",
    "\n",
    "cv = tx.CountVectorizer(vocabulary = top100)\n",
    "cv.fit(corpus['Message'])\n",
    "tr_xtrain = cv.transform(xtrain)\n",
    "tr_xtest = cv.transform(xtest)\n",
    "\n",
    "fitpredict(tr_xtrain, ytrain, tr_xtest, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- -- \n",
    "Используем tf-idf векторизатор:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfv = tx.TfidfVectorizer()\n",
    "tfidfv.fit(corpus['Message'])\n",
    "tr_xtrain = tfidfv.transform(xtrain)\n",
    "tr_xtest = tfidfv.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Логистическая регрессия: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.96004   0.99896   0.97911       962\n",
      "        spam    0.99123   0.73856   0.84644       153\n",
      "\n",
      "    accuracy                        0.96323      1115\n",
      "   macro avg    0.97563   0.86876   0.91278      1115\n",
      "weighted avg    0.96432   0.96323   0.96091      1115\n",
      " Случайный лес: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.97172   1.00000   0.98566       962\n",
      "        spam    1.00000   0.81699   0.89928       153\n",
      "\n",
      "    accuracy                        0.97489      1115\n",
      "   macro avg    0.98586   0.90850   0.94247      1115\n",
      "weighted avg    0.97560   0.97489   0.97380      1115\n",
      " Наивный Байес: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.98519   0.96778   0.97640       962\n",
      "        spam    0.81765   0.90850   0.86068       153\n",
      "\n",
      "    accuracy                        0.95964      1115\n",
      "   macro avg    0.90142   0.93814   0.91854      1115\n",
      "weighted avg    0.96220   0.95964   0.96052      1115\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(), RandomForestClassifier(), ComplementNB())"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitpredict(tr_xtrain, ytrain, tr_xtest, ytest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2a2d0a56c23e8ff240e12265c41bd4cf4ebc4a5bdfa5544f52816f127301f16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
