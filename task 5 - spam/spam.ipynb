{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn.feature_extraction.text as tx, sklearn.naive_bayes as nb, sklearn.linear_model as lm, sklearn.ensemble as en, sklearn.metrics as me, sklearn.model_selection as ms\n",
    "from warnings import filterwarnings as fw\n",
    "fw('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "все слова с учетом регистра<br>\n",
    "все в одном регистре<br>\n",
    "без стоп слов<br>\n",
    "выделение специфичных для классов<br>\n",
    "-- -- \n",
    "лог регрессия, случайный лес, байес"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('Н')%3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitpredict(xtrain,ytrain,xtest,ytest):\n",
    "    \n",
    "    lr =  lm.LogisticRegression()\n",
    "    rfc = en.RandomForestClassifier()\n",
    "    cnb = nb.ComplementNB()\n",
    "\n",
    "    lr.fit(xtrain,ytrain)\n",
    "    rfc.fit(xtrain,ytrain)\n",
    "    cnb.fit(xtrain,ytrain)\n",
    "\n",
    "    pred_lr = lr.predict(xtest)\n",
    "    pred_rfc = rfc.predict(xtest)\n",
    "    pred_cnb = cnb.predict(xtest)\n",
    "    \n",
    "    print('Логистическая регрессия: \\n', me.classification_report(ytest, pred_lr, digits=5),\n",
    "        'Случайный лес: \\n', me.classification_report(ytest, pred_rfc, digits=5),\n",
    "        'Наивный Байес: \\n', me.classification_report(ytest, pred_cnb, digits=5))\n",
    "    \n",
    "    return lr, rfc, cnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_csv('SMSpam.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего строк: 5572 \n",
      "Из них 747 spam и 4825 ham \n",
      "Всего слов:  87265 \n",
      "из них 17788 spam и 69477 ham\n"
     ]
    }
   ],
   "source": [
    "cnt, cntS, cntH = 0, 0, 0\n",
    "for i in corpus['Message']:\n",
    "    cnt += len(i.split(' '))\n",
    "for i in corpus[corpus['Category']=='spam']['Message']:\n",
    "    cntS += len(i.split(' '))\n",
    "for i in corpus[corpus['Category']=='ham']['Message']:\n",
    "    cntH += len(i.split(' '))\n",
    "print('Всего строк:', corpus.shape[0], '\\nИз них', \n",
    "    corpus['Category'].value_counts()['spam'], 'spam и',corpus['Category'].value_counts()['ham'], 'ham', \n",
    "    '\\nВсего слов: ', cnt, '\\nиз них', cntS, 'spam и', cntH, 'ham')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучающая выборка: (4457,) \n",
      "Тестовая выборка (1115,)\n"
     ]
    }
   ],
   "source": [
    "xtrain, xtest, ytrain, ytest = ms.train_test_split(corpus['Message'],corpus['Category'], test_size=0.2)\n",
    "print('Обучающая выборка:', xtrain.shape,'\\nТестовая выборка', xtest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- -- \n",
    "Используем все слова с учетом регистра:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего 9666 токенов\n"
     ]
    }
   ],
   "source": [
    "cv = tx.CountVectorizer(lowercase=False)\n",
    "cv.fit(xtrain)\n",
    "print('Всего', len(cv.vocabulary_.keys()), 'токенов')\n",
    "\n",
    "tr_xtrain = cv.transform(xtrain)\n",
    "tr_xtest = cv.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Логистическая регрессия: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.97840   0.99895   0.98857       952\n",
      "        spam    0.99301   0.87117   0.92810       163\n",
      "\n",
      "    accuracy                        0.98027      1115\n",
      "   macro avg    0.98570   0.93506   0.95834      1115\n",
      "weighted avg    0.98053   0.98027   0.97973      1115\n",
      " Случайный лес: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.97143   1.00000   0.98551       952\n",
      "        spam    1.00000   0.82822   0.90604       163\n",
      "\n",
      "    accuracy                        0.97489      1115\n",
      "   macro avg    0.98571   0.91411   0.94577      1115\n",
      "weighted avg    0.97561   0.97489   0.97389      1115\n",
      " Наивный Байес: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.98629   0.98214   0.98421       952\n",
      "        spam    0.89820   0.92025   0.90909       163\n",
      "\n",
      "    accuracy                        0.97309      1115\n",
      "   macro avg    0.94225   0.95119   0.94665      1115\n",
      "weighted avg    0.97341   0.97309   0.97323      1115\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(), RandomForestClassifier(), ComplementNB())"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitpredict(tr_xtrain, ytrain, tr_xtest, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- -- \n",
    "Используем все слова, но в одном регистре:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего 7762 токенов\n"
     ]
    }
   ],
   "source": [
    "cv = tx.CountVectorizer()\n",
    "cv.fit(xtrain)\n",
    "print('Всего', len(cv.vocabulary_.keys()), 'токенов')\n",
    "\n",
    "tr_xtrain = cv.transform(xtrain)\n",
    "tr_xtest = cv.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Логистическая регрессия: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.97837   0.99790   0.98804       952\n",
      "        spam    0.98611   0.87117   0.92508       163\n",
      "\n",
      "    accuracy                        0.97937      1115\n",
      "   macro avg    0.98224   0.93453   0.95656      1115\n",
      "weighted avg    0.97950   0.97937   0.97884      1115\n",
      " Случайный лес: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.97143   1.00000   0.98551       952\n",
      "        spam    1.00000   0.82822   0.90604       163\n",
      "\n",
      "    accuracy                        0.97489      1115\n",
      "   macro avg    0.98571   0.91411   0.94577      1115\n",
      "weighted avg    0.97561   0.97489   0.97389      1115\n",
      " Наивный Байес: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.98734   0.98319   0.98526       952\n",
      "        spam    0.90419   0.92638   0.91515       163\n",
      "\n",
      "    accuracy                        0.97489      1115\n",
      "   macro avg    0.94577   0.95479   0.95021      1115\n",
      "weighted avg    0.97519   0.97489   0.97501      1115\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(), RandomForestClassifier(), ComplementNB())"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitpredict(tr_xtrain, ytrain, tr_xtest, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- -- \n",
    "Найдем все стоп-слова, которые встречаются в большом количестве сообщений (более 500 , что около 9% всей выборки) и уберем их:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего 8 токенов\n",
      "{'my', 'me', 'in', 'is', 'you', 'the', 'and', 'to'}\n"
     ]
    }
   ],
   "source": [
    "cv = tx.CountVectorizer(max_df = 500)\n",
    "cv.fit(xtrain)\n",
    "stop = cv.stop_words_\n",
    "print('Всего', len(stop), 'токенов')\n",
    "print(stop)\n",
    "\n",
    "cv = tx.CountVectorizer(analyzer = 'word', stop_words = stop)\n",
    "cv.fit(xtrain)\n",
    "tr_xtrain = cv.transform(xtrain)\n",
    "tr_xtest = cv.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Логистическая регрессия: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.98142   0.99895   0.99011       952\n",
      "        spam    0.99315   0.88957   0.93851       163\n",
      "\n",
      "    accuracy                        0.98296      1115\n",
      "   macro avg    0.98729   0.94426   0.96431      1115\n",
      "weighted avg    0.98314   0.98296   0.98257      1115\n",
      " Случайный лес: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.97140   0.99895   0.98498       952\n",
      "        spam    0.99265   0.82822   0.90301       163\n",
      "\n",
      "    accuracy                        0.97399      1115\n",
      "   macro avg    0.98202   0.91359   0.94400      1115\n",
      "weighted avg    0.97451   0.97399   0.97300      1115\n",
      " Наивный Байес: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.98624   0.97899   0.98260       952\n",
      "        spam    0.88235   0.92025   0.90090       163\n",
      "\n",
      "    accuracy                        0.97040      1115\n",
      "   macro avg    0.93430   0.94962   0.94175      1115\n",
      "weighted avg    0.97106   0.97040   0.97066      1115\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(), RandomForestClassifier(), ComplementNB())"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitpredict(tr_xtrain, ytrain, tr_xtest, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдем все стоп-слова, которые встречаются в маленьком количестве сообщений (менее 10, что около 0.18% всей выборки) и уберем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего 6906 токенов отмечены как стоп-слова.\n",
      "Работаем с 7762 - 6906 = 856 токенами.\n"
     ]
    }
   ],
   "source": [
    "cv = tx.CountVectorizer(min_df = 10)\n",
    "cv.fit(xtrain)\n",
    "stop = cv.stop_words_\n",
    "print('Всего', len(stop), 'токенов отмечены как стоп-слова.')\n",
    "#print(stop) // не будем выводить список слов, так как он слишком большой\n",
    "\n",
    "cv = tx.CountVectorizer(analyzer = 'word', stop_words = stop)\n",
    "cv.fit(xtrain)\n",
    "tr_xtrain = cv.transform(xtrain)\n",
    "tr_xtest = cv.transform(xtest)\n",
    "print('Работаем с {0} - {1} = {2}'.format(len(cv.vocabulary_) + len(stop), len(stop), len(cv.vocabulary_)), 'токенами.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Логистическая регрессия: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.98240   0.99685   0.98957       952\n",
      "        spam    0.97987   0.89571   0.93590       163\n",
      "\n",
      "    accuracy                        0.98206      1115\n",
      "   macro avg    0.98113   0.94628   0.96273      1115\n",
      "weighted avg    0.98203   0.98206   0.98173      1115\n",
      " Случайный лес: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.97639   0.99895   0.98754       952\n",
      "        spam    0.99291   0.85890   0.92105       163\n",
      "\n",
      "    accuracy                        0.97848      1115\n",
      "   macro avg    0.98465   0.92892   0.95430      1115\n",
      "weighted avg    0.97880   0.97848   0.97782      1115\n",
      " Наивный Байес: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.98816   0.96429   0.97608       952\n",
      "        spam    0.81720   0.93252   0.87106       163\n",
      "\n",
      "    accuracy                        0.95964      1115\n",
      "   macro avg    0.90268   0.94840   0.92357      1115\n",
      "weighted avg    0.96317   0.95964   0.96072      1115\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(), RandomForestClassifier(), ComplementNB())"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitpredict(tr_xtrain, ytrain, tr_xtest, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем встроенный список стоп-слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Логистическая регрессия: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.97641   1.00000   0.98806       952\n",
      "        spam    1.00000   0.85890   0.92409       163\n",
      "\n",
      "    accuracy                        0.97937      1115\n",
      "   macro avg    0.98821   0.92945   0.95608      1115\n",
      "weighted avg    0.97986   0.97937   0.97871      1115\n",
      " Случайный лес: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.97140   0.99895   0.98498       952\n",
      "        spam    0.99265   0.82822   0.90301       163\n",
      "\n",
      "    accuracy                        0.97399      1115\n",
      "   macro avg    0.98202   0.91359   0.94400      1115\n",
      "weighted avg    0.97451   0.97399   0.97300      1115\n",
      " Наивный Байес: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.99237   0.95693   0.97433       952\n",
      "        spam    0.79188   0.95706   0.86667       163\n",
      "\n",
      "    accuracy                        0.95695      1115\n",
      "   macro avg    0.89213   0.95699   0.92050      1115\n",
      "weighted avg    0.96306   0.95695   0.95859      1115\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(), RandomForestClassifier(), ComplementNB())"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = tx.CountVectorizer(stop_words = 'english')\n",
    "cv.fit(xtrain)\n",
    "tr_xtrain = cv.transform(xtrain)\n",
    "tr_xtest = cv.transform(xtest)\n",
    "fitpredict(tr_xtrain, ytrain, tr_xtest, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- -- \n",
    "Используем только слова с высокой важностью:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9739910313901345"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = tx.CountVectorizer()\n",
    "cv.fit(xtrain)\n",
    "tr_xtrain = cv.transform(xtrain)\n",
    "tr_xtest = cv.transform(xtest)\n",
    "\n",
    "e = en.RandomForestClassifier()\n",
    "e.fit(tr_xtrain,ytrain)\n",
    "e.score(tr_xtest,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Логистическая регрессия: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.95762   0.99685   0.97684       952\n",
      "        spam    0.97581   0.74233   0.84321       163\n",
      "\n",
      "    accuracy                        0.95964      1115\n",
      "   macro avg    0.96671   0.86959   0.91002      1115\n",
      "weighted avg    0.96028   0.95964   0.95730      1115\n",
      " Случайный лес: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.96633   0.99475   0.98033       952\n",
      "        spam    0.96296   0.79755   0.87248       163\n",
      "\n",
      "    accuracy                        0.96592      1115\n",
      "   macro avg    0.96464   0.89615   0.92641      1115\n",
      "weighted avg    0.96583   0.96592   0.96457      1115\n",
      " Наивный Байес: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.97730   0.94958   0.96324       952\n",
      "        spam    0.74737   0.87117   0.80453       163\n",
      "\n",
      "    accuracy                        0.93812      1115\n",
      "   macro avg    0.86233   0.91037   0.88389      1115\n",
      "weighted avg    0.94368   0.93812   0.94004      1115\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(), RandomForestClassifier(), ComplementNB())"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp = pd.DataFrame({'name' : cv.vocabulary_.keys(), 'importance' : e.feature_importances_}) \n",
    "imp = imp.sort_values(by = 'importance', ascending = False)\n",
    "top100 = imp.head(n=1000)['name'].to_numpy()\n",
    "\n",
    "cv = tx.CountVectorizer(vocabulary = top100)\n",
    "cv.fit(xtrain)\n",
    "tr_xtrain = cv.transform(xtrain)\n",
    "tr_xtest = cv.transform(xtest)\n",
    "\n",
    "fitpredict(tr_xtrain, ytrain, tr_xtest, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- -- \n",
    "Используем tf-idf векторизатор слов в одном регистре:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfv = tx.TfidfVectorizer()\n",
    "tfidfv.fit(corpus['Message'])\n",
    "tr_xtrain = tfidfv.transform(xtrain)\n",
    "tr_xtest = tfidfv.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Логистическая регрессия: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.95010   1.00000   0.97441       952\n",
      "        spam    1.00000   0.69325   0.81884       163\n",
      "\n",
      "    accuracy                        0.95516      1115\n",
      "   macro avg    0.97505   0.84663   0.89663      1115\n",
      "weighted avg    0.95739   0.95516   0.95167      1115\n",
      " Случайный лес: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.97044   1.00000   0.98500       952\n",
      "        spam    1.00000   0.82209   0.90236       163\n",
      "\n",
      "    accuracy                        0.97399      1115\n",
      "   macro avg    0.98522   0.91104   0.94368      1115\n",
      "weighted avg    0.97476   0.97399   0.97292      1115\n",
      " Наивный Байес: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham    0.98512   0.97374   0.97940       952\n",
      "        spam    0.85632   0.91411   0.88427       163\n",
      "\n",
      "    accuracy                        0.96502      1115\n",
      "   macro avg    0.92072   0.94392   0.93184      1115\n",
      "weighted avg    0.96629   0.96502   0.96549      1115\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(), RandomForestClassifier(), ComplementNB())"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitpredict(tr_xtrain, ytrain, tr_xtest, ytest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2a2d0a56c23e8ff240e12265c41bd4cf4ebc4a5bdfa5544f52816f127301f16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
